# 벡터화 방식 설명

## 📊 현재 파이프라인에서 사용하는 벡터화 방식

이 프로젝트에서는 **두 가지 벡터화 방식**을 사용합니다:

---

## 1️⃣ TF-IDF 벡터화 (1단계, 2단계)

### 사용 위치
- **1단계**: `1_형태소분석_TFIDF.py`
- **2단계**: `2_덴드로그램_시각화.py`

### 벡터화 과정

```
원본 텍스트
    ↓
형태소 분석 (명사, 동사, 형용사 추출)
    ↓
키워드 리스트 생성
    ↓
TF-IDF 벡터화
    ↓
Sparse Matrix (희소 행렬)
```

### 상세 설명

1. **형태소 분석**
   - `MorphologicalAnalyzer`를 사용하여 텍스트를 형태소로 분해
   - 명사(NNG, NNP), 동사(VV), 형용사(VA) 등 추출
   - 예: "안녕하세요 반갑습니다" → ["안녕", "반갑"]

2. **TF-IDF 계산**
   - **TF (Term Frequency)**: 문서 내 단어 출현 빈도
   - **IDF (Inverse Document Frequency)**: 역문서 빈도 (전체 문서에서의 희귀도)
   - **공식**: `TF-IDF = TF × IDF`
   - 자주 나오지만 특정 문서에만 나오는 단어에 높은 가중치 부여

3. **벡터화 결과**
   - **타입**: Sparse Matrix (scipy.sparse.csr_matrix)
   - **크기**: (문서 수, 단어 수)
   - **특징**: 대부분의 값이 0인 희소 행렬
   - **용도**: 덴드로그램 시각화, 단어 기반 분석

### 코드 예시

```python
from tfidf_analysis import TFIDFAnalyzer

# 형태소 분석 후 키워드 리스트
keywords_list = [['안녕', '반갑'], ['좋은', '하루']]

# TF-IDF 벡터화
tfidf_analyzer = TFIDFAnalyzer()
tokenized_texts = [' '.join(keywords) for keywords in keywords_list]
tfidf_matrix = tfidf_analyzer.fit_transform(tokenized_texts)

# 결과: Sparse Matrix
# shape: (문서 수, 단어 수)
# 예: (1000, 5000) → 1000개 문서, 5000개 고유 단어
```

### 장점
- ✅ 단어의 중요도를 수치화
- ✅ 희소 행렬로 메모리 효율적
- ✅ 해석 가능 (어떤 단어가 중요한지 알 수 있음)

### 단점
- ❌ 단어 순서 정보 손실
- ❌ 의미적 유사도 반영 어려움
- ❌ 동의어 처리 불가

---

## 2️⃣ SentenceTransformer 임베딩 (3단계)

### 사용 위치
- **3단계**: `3_BERTopic_클러스터링.py`

### 벡터화 과정

```
원본 텍스트
    ↓
SentenceTransformer 모델
(jhgan/ko-sroberta-multitask)
    ↓
Dense Vector (밀집 벡터)
    ↓
UMAP 차원 축소
    ↓
최종 임베딩
```

### 상세 설명

1. **SentenceTransformer 모델**
   - **모델**: `jhgan/ko-sroberta-multitask`
   - **타입**: 한국어 RoBERTa 기반 SentenceTransformer
   - **입력**: 원본 텍스트 (형태소 분석 불필요)
   - **출력**: 768차원 dense vector (또는 모델에 따라 다름)

2. **임베딩 생성**
   - 전체 문장/문서를 하나의 벡터로 변환
   - 문맥 정보를 고려한 의미적 표현
   - 유사한 의미의 문장은 유사한 벡터로 표현

3. **차원 축소 (UMAP)**
   - **UMAP**: Uniform Manifold Approximation and Projection
   - **입력**: 768차원 임베딩
   - **출력**: 5차원 벡터 (기본값)
   - **목적**: 클러스터링을 위한 차원 축소

4. **벡터화 결과**
   - **타입**: Dense Matrix (numpy.ndarray)
   - **크기**: (문서 수, 임베딩 차원)
   - **특징**: 모든 값이 실수 (0이 아닌 값)
   - **용도**: BERTopic 클러스터링

### 코드 예시

```python
from sentence_transformers import SentenceTransformer

# SentenceTransformer 모델 로드
model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# 원본 텍스트 리스트
documents = ["안녕하세요 반갑습니다", "좋은 하루 되세요"]

# 임베딩 생성
embeddings = model.encode(documents)

# 결과: Dense Matrix
# shape: (문서 수, 임베딩 차원)
# 예: (1000, 768) → 1000개 문서, 768차원 벡터
```

### 장점
- ✅ 의미적 유사도 반영
- ✅ 문맥 정보 보존
- ✅ 동의어 자동 처리
- ✅ 문장 전체 의미 파악

### 단점
- ❌ 해석 어려움 (벡터의 각 차원 의미 불명확)
- ❌ 메모리 사용량 큼 (dense matrix)
- ❌ 계산 비용 높음 (GPU 권장)

---

## 🔄 두 방식의 차이점 비교

| 항목 | TF-IDF | SentenceTransformer |
|------|--------|---------------------|
| **입력** | 형태소 분석된 키워드 | 원본 텍스트 |
| **출력 타입** | Sparse Matrix | Dense Matrix |
| **차원** | 단어 수 (수천~수만) | 고정 차원 (768) |
| **의미 반영** | ❌ | ✅ |
| **해석 가능성** | ✅ | ❌ |
| **계산 속도** | 빠름 | 느림 (GPU 권장) |
| **메모리** | 효율적 | 많이 사용 |
| **용도** | 단어 기반 분석 | 의미 기반 클러스터링 |

---

## 📍 각 단계에서의 사용

### 1단계: 형태소 분석 및 TF-IDF
- **벡터화**: TF-IDF
- **목적**: 단어 빈도 분석, 상위 특성 추출
- **결과**: `tfidf_matrix.pkl` 저장

### 2단계: 덴드로그램 시각화
- **벡터화**: TF-IDF (1단계에서 생성)
- **목적**: 계층적 클러스터링 시각화
- **결과**: `덴드로그램.png`

### 3단계: BERTopic 클러스터링
- **벡터화**: SentenceTransformer 임베딩
- **목적**: 의미 기반 토픽 모델링
- **결과**: `문서별_토픽할당.csv`, `토픽요약정보.csv`

### 4단계: 감정분석
- **벡터화**: 사용 안 함 (분류 모델 직접 사용)
- **모델**: KcELECTRA (시퀀스 분류)
- **결과**: `감정분석_결과.csv`

### 5단계: CAM 기회영역 시각화
- **벡터화**: 사용 안 함 (통계 분석)
- **입력**: 토픽 할당, 감정분석 결과
- **결과**: `기회영역_분석.csv`

---

## 💡 왜 두 가지 방식을 사용하나요?

1. **TF-IDF**: 
   - 단어 기반 분석에 적합
   - 해석 가능한 결과
   - 빠른 계산

2. **SentenceTransformer**:
   - 의미 기반 클러스터링에 적합
   - 문맥 정보 활용
   - 더 정확한 토픽 발견

두 방식을 함께 사용하여 **다각도 분석**이 가능합니다!

