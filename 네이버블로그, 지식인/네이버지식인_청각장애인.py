# --- ë°ì´í„° ìˆ˜ì§‘(Crawling)ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from selenium import webdriver as wb
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException, TimeoutException  # type: ignore
import re  # ì •ê·œì‹ ì‚¬ìš©
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup as bs  # BeautifulSoup ì¶”ê°€

# --- ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
import pandas as pd
import time
from datetime import datetime, timedelta
from urllib.parse import quote, urlparse, parse_qs, unquote
import pickle
import sys

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass


def smart_scroll(driver, scroll_step=1200, pause=0.5, container_selectors=None):
    """
    ì§€ì‹ì¸ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì— ë§ì¶˜ ë‹¤ì¤‘ ìŠ¤í¬ë¡¤ íŠ¸ë¦¬ê±°
    """
    container_selectors = container_selectors or ['#main_pack', '#content', '#wrap', '#container', 'body', 'html']
    
    scroll_scripts = [
        ("window.scrollBy(0, arguments[0]);", scroll_step),
        ("window.scrollTo(0, document.body.scrollHeight);", None),
        ("window.scrollTo(0, document.documentElement.scrollHeight);", None)
    ]
    
    for script, value in scroll_scripts:
        try:
            if value is None:
                driver.execute_script(script)
            else:
                driver.execute_script(script, value)
            time.sleep(pause * 0.35)
        except Exception:
            continue
    
    for selector in container_selectors:
        try:
            driver.execute_script("""
                const target = document.querySelector(arguments[0]);
                const delta = arguments[1];
                if (!target) { return; }
                if (target === document.body || target === document.documentElement) {
                    window.scrollBy(0, delta);
                    window.scrollTo(0, target.scrollHeight);
                } else {
                    target.scrollTop = target.scrollHeight;
                    target.dispatchEvent(new Event('scroll', {bubbles: true}));
                }
            """, selector, scroll_step)
            time.sleep(pause * 0.2)
        except Exception:
            continue
    
    try:
        driver.execute_script("""
            window.dispatchEvent(new Event('scroll', {bubbles: true}));
            if (typeof WheelEvent !== 'undefined') {
                window.dispatchEvent(new WheelEvent('wheel', {deltaY: arguments[0], bubbles: true}));
            }
        """, scroll_step)
        time.sleep(pause * 0.15)
    except Exception:
        pass
    
    try:
        return driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);")
    except Exception:
        return None


def recover_scroll(driver, scroll_step=1200, pause=0.5):
    """
    ìŠ¤í¬ë¡¤ ì •ì²´ ì‹œ ë‹¤ì‹œ ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œì¼œ ì»¨í…ì¸  ë¡œë”©ì„ ìœ ë„
    """
    try:
        driver.execute_script("window.scrollBy(0, -300);")
        time.sleep(pause * 0.4)
        driver.execute_script("window.scrollBy(0, arguments[0]);", max(400, scroll_step // 2))
        time.sleep(pause * 0.4)
    except Exception:
        pass


def extract_real_kin_url(raw_url):
    """
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–»ì€ href(raw_url)ì—ì„œ ì‹¤ì œ kin.naver.com URLì„ ë½‘ì•„ëƒ„
    - search.naver.comì˜ redirect URL (u=...)ë„ ì²˜ë¦¬
    """
    if not raw_url:
        return None
    
    url = raw_url.strip()
    
    # 1) search.naver.com ... u= ì‹¤ì œ URL ì¸ì½”ë”© í˜•íƒœ
    if 'search.naver.com' in url and 'u=' in url and 'kin.naver.com' not in url:
        try:
            parsed = urlparse(url)
            qs = parse_qs(parsed.query)
            if 'u' in qs and qs['u']:
                target = unquote(qs['u'][0])
                url = target
        except Exception:
            pass
    
    # 2) ì‹¤ì œ ì§€ì‹ì¸ URLì¸ì§€ í™•ì¸
    if 'kin.naver.com' not in url:
        return None

    # 3) ëª…ë°±íˆ ì´ìƒí•œ URL(í”„ë¡œí•„ ë§í¬ ë“±) ì œê±°
    if 'search/profileLink' in url or '/profileLink' in url:
        return None
    
    # #, ë¶ˆí•„ìš”í•œ íŠ¸ë˜í‚¹ íŒŒë¼ë¯¸í„° ë“± ì •ë¦¬
    url = url.split('#')[0].strip()
    return url


def collect_kin_urls(driver, kin_url_pattern):
    """
    í˜„ì¬ í˜ì´ì§€ì—ì„œ ì§€ì‹ì¸ URLì„ ìˆ˜ì§‘
    ë…¸íŠ¸ë¶ ë°©ì‹(BeautifulSoup) ì¶”ê°€
    """
    temp_urls = set()
    
    # ë°©ë²• 0: ë…¸íŠ¸ë¶ ë°©ì‹ - BeautifulSoup ì‚¬ìš© (div.question_area > div:nth-child(3) > a)
    try:
        html = driver.page_source
        soup = bs(html, 'lxml')
        url_tags = soup.select('div.question_area > div:nth-child(3) > a')
        print(f"  ğŸ” BeautifulSoupìœ¼ë¡œ ì°¾ì€ URL íƒœê·¸ ê°œìˆ˜: {len(url_tags)}ê°œ")
        
        for tag in url_tags:
            try:
                href = tag.get('href')
                if not href:
                    continue
                real_url = extract_real_kin_url(href)
                if not real_url:
                    continue
                # qna/detail, qna/questionë§Œ ë‚¨ê¸°ê¸°
                if kin_url_pattern.search(real_url):
                    temp_urls.add(real_url)
            except Exception:
                continue
    except Exception:
        pass
    
    # ë°©ë²• 1: ìƒˆ UI (headline2) ê¸°ë°˜ ì¶”ì¶œ
    try:
        headline_spans = driver.find_elements(
            By.CSS_SELECTOR,
            "span.sds-comps-text.sds-comps-text-ellipsis-1.sds-comps-text-type-headline2"
        )
        print(f"  ğŸ” headline2 span ê°œìˆ˜: {len(headline_spans)}ê°œ")
        
        for span in headline_spans:
            try:
                link_el = span.find_element(By.XPATH, "./ancestor::a[1]")
                href = link_el.get_attribute('href') or ''
                if not href:
                    continue
                
                real_url = extract_real_kin_url(href)
                if not real_url:
                    continue
                
                if kin_url_pattern.search(real_url):
                    temp_urls.add(real_url)
            except Exception:
                continue
    except Exception:
        pass
    
    # ë°©ë²• 2: ê¸°ì¡´ title_link í´ë˜ìŠ¤ ê¸°ë°˜
    try:
        title_links = driver.find_elements(By.CLASS_NAME, 'title_link')
        for link in title_links:
            try:
                href = link.get_attribute('href')
                if not href:
                    continue
                real_url = extract_real_kin_url(href)
                if not real_url:
                    continue
                if kin_url_pattern.search(real_url):
                    temp_urls.add(real_url)
            except Exception:
                continue
    except Exception:
        pass
    
    # ë°©ë²• 3: a[href*="kin.naver.com"] ê¸°ë°˜ ë³´ì™„
    try:
        kin_links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="kin.naver.com"], a[href*="kin.naver.com/qna"]')
        for link in kin_links:
            try:
                href = link.get_attribute('href')
                if not href:
                    continue
                real_url = extract_real_kin_url(href)
                if not real_url:
                    continue
                if kin_url_pattern.search(real_url):
                    temp_urls.add(real_url)
            except Exception:
                continue
    except Exception:
        pass
    
    return temp_urls


def calculate_date_ranges(start_date_str='20230101', end_date_str='20251114', max_urls=2500):
    """
    ë‚ ì§œ ë²”ìœ„ ê³„ì‚° - 2ë…„ ë‹¨ìœ„
    """
    start_date = datetime.strptime(start_date_str, '%Y%m%d')
    end_date = datetime.strptime(end_date_str, '%Y%m%d')
    date_ranges = []
    
    current_start = start_date
    while current_start < end_date:
        current_end_2y = current_start + timedelta(days=730)
        if current_end_2y > end_date:
            current_end_2y = end_date
        
        date_ranges.append({
            'start': current_start.strftime('%Y%m%d'),
            'end': current_end_2y.strftime('%Y%m%d'),
            'period': '2years'
        })
        
        current_start = current_end_2y + timedelta(days=1)
        if current_start >= end_date:
            break
    
    return date_ranges


def crawl_naver_kin(keyword, start_date, end_date, max_urls=2500):
    """
    ë„¤ì´ë²„ ì§€ì‹ì¸ í¬ë¡¤ë§
    """
    full_keyword = keyword
    keyword = parse_keyword_for_display(full_keyword)
    
    print(f"\n{'='*60}")
    print(f"ë„¤ì´ë²„ ì§€ì‹ì¸ í¬ë¡¤ë§ ì‹œì‘")
    print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {full_keyword}")
    print(f"ì €ì¥ í‚¤ì›Œë“œ: {keyword}")
    print(f"ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"ìµœëŒ€ URL ìˆ˜: {max_urls}")
    print(f"{'='*60}\n")
    
    encoded_keyword = quote(full_keyword, safe='')
    kin_search_url = f'https://search.naver.com/search.naver?where=kin&query={encoded_keyword}&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom{start_date}to{end_date}'
    
    print(f"ìƒì„±ëœ URL: {kin_search_url}\n")
    
    options = wb.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-software-rasterizer')
    options.add_argument('--disable-infobars')
    options.add_argument('--log-level=3')
    options.add_argument('--disable-logging')
    options.add_argument('--disable-background-networking')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-breakpad')
    options.add_argument('--disable-component-update')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-sync')
    options.add_argument('--disable-background-mode')
    options.add_argument('--disable-features=TranslateUI')
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option('prefs', {
        'profile.default_content_setting_values.notifications': 2,
        'profile.default_content_settings.popups': 0,
    })
    options.add_experimental_option('useAutomationExtension', False)
    options.page_load_strategy = 'normal'
    
    try:
        driver = wb.Chrome(options=options)
    except Exception as e:
        print(f"âš ï¸  Chrome ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹¤íŒ¨, ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„: {e}")
        driver = wb.Chrome()
    
    driver.maximize_window()
    driver.implicitly_wait(10)
    
    # ğŸ”¥ qna/detail ë˜ëŠ” qna/questionë§Œ í—ˆìš©
    kin_url_pattern = re.compile(
        r'https?://kin\.naver\.com/qna/(detail|question)\.naver[^"\'\s]*'
    )
    
    try:
        driver.get(kin_search_url)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        print("âœ… ë¸Œë¼ìš°ì € ì‹¤í–‰ ë° í˜ì´ì§€ ì´ë™ ì™„ë£Œ!")
        
        print("\nğŸ” 'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ ì°¾ëŠ” ì¤‘...")
        time.sleep(2)
        
        try:
            detail_buttons = driver.find_elements(
                By.CSS_SELECTOR,
                'a.more_link, a[class*="more_link"], a[onclick*="goOtherCR"], a[href*="where=kin"]'
            )
            
            clicked = False
            for btn in detail_buttons:
                try:
                    if btn.is_displayed() and btn.is_enabled():
                        btn_text = btn.text.strip()
                        btn_class = btn.get_attribute('class') or ''
                        if 'ìƒì„¸' in btn_text or 'ìƒì„¸ ê²€ìƒ‰' in btn_text or 'more_link' in btn_class:
                            driver.execute_script("arguments[0].click();", btn)
                            print("âœ… 'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ!")
                            time.sleep(3)
                            clicked = True
                            break
                except Exception:
                    continue
            
            if not clicked:
                print("âš ï¸  'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰...")
        except Exception as e:
            print(f"âš ï¸  ë²„íŠ¼ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}. ê³„ì† ì§„í–‰...")
        
        print(f"\n{'='*60}")
        print("ìŠ¤í¬ë¡¤ ê¸°ë°˜ URL ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*60}\n")
        
        # ìŠ¤í¬ë¡¤ ì„¤ì •
        max_scroll_count = 150
        scroll_pause = 1.5
        no_change_limit = 10
        
        time.sleep(3)
        
        try:
            body = driver.find_element(By.TAG_NAME, 'body')
        except Exception:
            body = None
        
        last_height = driver.execute_script(
            "return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight)"
        )
        no_change_count = 0
        actual_scrolls = 0
        
        print("  ğŸ“œ ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì¤‘...")
        for scroll_round in range(max_scroll_count):
            actual_scrolls = scroll_round + 1
            
            try:
                if body is None:
                    body = driver.find_element(By.TAG_NAME, 'body')
                body.send_keys(Keys.END)
            except Exception:
                try:
                    body = driver.find_element(By.TAG_NAME, 'body')
                    body.send_keys(Keys.END)
                except Exception:
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            time.sleep(scroll_pause)
            
            if scroll_round % 5 == 0:
                try:
                    smart_scroll(driver, scroll_step=1500, pause=0.8)
                    time.sleep(0.5)
                except Exception:
                    pass
            
            new_height = driver.execute_script(
                "return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight)"
            )
            
            if new_height == last_height:
                no_change_count += 1
                if no_change_count >= no_change_limit:
                    print(f"  âš ï¸  ìŠ¤í¬ë¡¤ ë†’ì´ ë³€í™” ì—†ìŒ ({actual_scrolls}ë²ˆì§¸ ìŠ¤í¬ë¡¤, {no_change_count}íšŒ ì—°ì†). ì¡°ê¸° ì¢…ë£Œ.")
                    break
            else:
                no_change_count = 0
                last_height = new_height
            
            if (scroll_round + 1) % 10 == 0:
                print(f"    ì§„í–‰: {scroll_round + 1}/{max_scroll_count}ë²ˆ ìŠ¤í¬ë¡¤ ì™„ë£Œ (í˜„ì¬ ë†’ì´: {new_height})")
                try:
                    more_buttons = driver.find_elements(By.CSS_SELECTOR, 'a.more, button.more, .more, [class*="more"], [class*="btn_more"]')
                    for btn in more_buttons:
                        try:
                            if btn.is_displayed() and btn.is_enabled():
                                driver.execute_script("arguments[0].click();", btn)
                                time.sleep(1.5)
                                print("  ğŸ”„ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­")
                                break
                        except Exception:
                            continue
                except Exception:
                    pass
        
        print(f"  âœ… ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì™„ë£Œ! (ì´ {actual_scrolls}ë²ˆ ìŠ¤í¬ë¡¤)")
        
        print("  â³ ìµœì¢… ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(3)
        
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            smart_scroll(driver, scroll_step=2000, pause=1.0)
            time.sleep(2)
        except Exception:
            pass
        
        print("  ğŸ” URL ìˆ˜ì§‘ ì¤‘...")
        all_seen_urls = collect_kin_urls(driver, kin_url_pattern)
        href_list = list(all_seen_urls)
        
        print(f"\nâœ… ì´ {len(href_list)}ê°œì˜ ì§€ì‹ì¸ URL ìˆ˜ì§‘ ì™„ë£Œ! (ì´ {actual_scrolls}ë²ˆ ìŠ¤í¬ë¡¤)")
        
        if len(href_list) < 1000:
            print(f"\nâš ï¸  URL ìˆ˜ê°€ 1000ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤ ({len(href_list)}ê°œ).")
            print("   1ë…„ ë‹¨ìœ„ë¡œ ê¸°ê°„ì„ í™•ì¥í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if len(href_list) > max_urls:
            href_list = href_list[:max_urls]
            print(f"âš ï¸  ìµœëŒ€ URL ìˆ˜({max_urls})ë¡œ ì œí•œ: {len(href_list)}ê°œ")
        
        print(f"\n{'='*60}")
        print("ì§€ì‹ì¸ ì§ˆë¬¸ ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
        print(f"{'='*60}\n")
        
        all_data = []
        seen_urls = set()
        
        for i, url in enumerate(href_list, 1):
            try:
                if not url or url.strip() == "":
                    print(f"[{i}/{len(href_list)}] â­ï¸  ìœ íš¨í•˜ì§€ ì•Šì€ URLë¡œ ê±´ë„ˆëœ€: {url}")
                    continue
                
                if url in seen_urls:
                    print(f"[{i}/{len(href_list)}] â­ï¸  ì¤‘ë³µ URLë¡œ ê±´ë„ˆëœ€: {url[:60]}...")
                    continue
                
                # qna/detail, qna/questionë§Œ
                if '/qna/' not in url:
                    print(f"[{i}/{len(href_list)}] â­ï¸  qna í˜ì´ì§€ê°€ ì•„ë‹ˆë¼ ê±´ë„ˆëœ€: {url[:60]}...")
                    continue
                if 'search/profileLink' in url or '/profileLink' in url:
                    print(f"[{i}/{len(href_list)}] â­ï¸  í”„ë¡œí•„ ë§í¬ ê±´ë„ˆëœ€: {url[:60]}...")
                    continue
                
                seen_urls.add(url)
                print(f"[{i}/{len(href_list)}] ì²˜ë¦¬ ì¤‘: {url[:60]}...")
                driver.get(url)
                time.sleep(2)
                
                # -----------------------------
                # ì œëª© ì¶”ì¶œ (ë…¸íŠ¸ë¶ ë°©ì‹ + Selenium)
                # -----------------------------
                title = "N/A"
                
                # 1ìˆœìœ„: <title>ì—ì„œ ì¶”ì¶œ
                try:
                    html = driver.page_source
                    soup = bs(html, 'lxml')
                    title_elem = soup.select_one('title')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        title = title.replace(': ì§€ì‹iN', '').strip()
                except Exception:
                    pass
                
                # 2ìˆœìœ„: Selenium ì…€ë ‰í„°
                if title == "N/A" or not title:
                    title_selectors = [
                        (By.CSS_SELECTOR, '.title'),
                        (By.CSS_SELECTOR, '.question-title'),
                        (By.CSS_SELECTOR, '.c-heading__title'),
                        (By.CSS_SELECTOR, 'h2.title'),
                        (By.TAG_NAME, 'h2'),
                        (By.TAG_NAME, 'h1')
                    ]
                    
                    for selector_type, selector in title_selectors:
                        try:
                            title_elem = driver.find_element(selector_type, selector)
                            t = title_elem.text.strip()
                            if t:
                                title = t
                                break
                        except:
                            continue
                
                # -----------------------------
                # ë‚´ìš©(content) ì¶”ì¶œ
                # -----------------------------
                content = ""
                
                # 1ìˆœìœ„: BeautifulSoupìœ¼ë¡œ .questionDetail
                try:
                    html = driver.page_source
                    soup = bs(html, 'lxml')
                    question_detail = soup.select_one('.questionDetail')
                    if question_detail:
                        tmp = question_detail.get_text(" ", strip=True)
                        if tmp:
                            content = tmp
                except Exception:
                    pass
                
                # 2ìˆœìœ„: .questionDetail ì•ˆì˜ p.se-text-paragraph (Selenium)
                if not content:
                    try:
                        detail = driver.find_element(By.CSS_SELECTOR, '.questionDetail')
                        paragraphs = detail.find_elements(By.CSS_SELECTOR, 'p.se-text-paragraph')
                        texts = []
                        for p in paragraphs:
                            txt = p.text.strip()
                            if txt:
                                texts.append(txt)
                        if texts:
                            content = ' '.join(texts)
                    except Exception:
                        pass
                
                # 3ìˆœìœ„: ê¸°ì¡´ selector ë°±ì—… (ì§ˆë¬¸ ë³¸ë¬¸)
                if not content:
                    try:
                        content_selectors = [
                            (By.CSS_SELECTOR, '.c-heading__content'),
                            (By.CSS_SELECTOR, '.question-content'),
                            (By.CSS_SELECTOR, '.content'),
                            (By.CSS_SELECTOR, '.question_text'),
                            (By.CSS_SELECTOR, '#answer-content')
                        ]
                        
                        for selector_type, selector in content_selectors:
                            try:
                                content_elem = driver.find_element(selector_type, selector)
                                tmp = content_elem.text.strip()
                                tmp = tmp.replace('\n', ' ').replace('\r', ' ')
                                tmp = ' '.join(tmp.split())
                                if tmp:
                                    content = tmp
                                    break
                            except:
                                continue
                    except Exception:
                        pass
                
                # 4ìˆœìœ„: BeautifulSoupìœ¼ë¡œ ì£¼ìš” ì˜ì—­ í…ìŠ¤íŠ¸ í•œ ë²ˆ ë” ì‹œë„
                if not content:
                    try:
                        html = driver.page_source
                        soup = bs(html, 'lxml')
                        for selector in ['.c-heading__content', '.question-content', '.content', '.question_text']:
                            elem = soup.select_one(selector)
                            if elem:
                                tmp = elem.get_text(" ", strip=True)
                                if tmp:
                                    content = ' '.join(tmp.split())
                                    break
                    except Exception:
                        pass
                
                # 5ìˆœìœ„: ìµœí›„ì˜ ë³´ë£¨ - í˜ì´ì§€ ì£¼ìš” ì˜ì—­ ì „ì²´ í…ìŠ¤íŠ¸
                if not content:
                    try:
                        html = driver.page_source
                        soup = bs(html, 'lxml')
                        main = (
                            soup.select_one('div#content')
                            or soup.select_one('div#main_content')
                            or soup.body
                        )
                        if main:
                            tmp = main.get_text(" ", strip=True)
                            if tmp:
                                content = ' '.join(tmp.split())
                    except Exception:
                        pass
                
                # content í›„ì²˜ë¦¬
                if content:
                    content = content.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
                    content = ' '.join(content.split())
                
                # -----------------------------
                # ë‚ ì§œ(date) ì¶”ì¶œ
                # -----------------------------
                date = "N/A"
                
                # 1ìˆœìœ„: BeautifulSoup
                try:
                    html = driver.page_source
                    soup = bs(html, 'lxml')
                    try:
                        date_elem = soup.select_one('div.userInfo.userInfo__bullet > span:nth-child(3)')
                        if date_elem:
                            date = date_elem.get_text(strip=True)
                            date = date.replace('ì‘ì„±ì¼', '').strip()
                        else:
                            date_elem = soup.select_one('div.userInfo.userInfo__bullet > span:nth-child(2)')
                            if date_elem:
                                date = date_elem.get_text(strip=True)
                                date = date.replace('ì‘ì„±ì¼', '').strip()
                    except:
                        pass
                except Exception:
                    pass
                
                # 2ìˆœìœ„: Selenium
                if date == "N/A" or not date:
                    date_selectors = [
                        (By.CSS_SELECTOR, 'div.userInfo.userInfo__bullet > span:nth-child(3)'),
                        (By.CSS_SELECTOR, 'div.userInfo.userInfo__bullet > span:nth-child(2)'),
                        (By.CSS_SELECTOR, '.c-userinfo__date'),
                        (By.CSS_SELECTOR, '.question-date'),
                        (By.CSS_SELECTOR, '.date'),
                        (By.CSS_SELECTOR, '.c-heading__date')
                    ]
                    
                    for selector_type, selector in date_selectors:
                        try:
                            date_elem = driver.find_element(selector_type, selector)
                            d = date_elem.text.strip()
                            if d and d != "N/A":
                                d = d.replace('ì‘ì„±ì¼', '').strip()
                                if '.' in d:
                                    date_parts = d.split('.')
                                    if len(date_parts) >= 3:
                                        d = f"{date_parts[0]}.{date_parts[1]}.{date_parts[2]}."
                                date = d
                                break
                        except:
                            continue
                
                # -----------------------------
                # ìµœì¢… í•„í„°ë§
                # -----------------------------
                # âœ¨ ìš”êµ¬ì‚¬í•­: "ë‚´ìš© ì¢€ ì§§ì•„ë„ ê´œì°®ê³ , N/Aë§Œ ì•„ë‹ˆë©´ ê¸ì–´ì˜¤ê¸°"
                if not content or content.strip() == "":
                    print(f"  â­ï¸  content ì—†ìŒìœ¼ë¡œ ê±´ë„ˆëœ€: {title[:30]}...")
                    continue
                
                if title == "N/A" or not title or len(title.strip()) == 0:
                    print(f"  â­ï¸  title ì—†ìŒìœ¼ë¡œ ê±´ë„ˆëœ€")
                    continue
                
                all_data.append({
                    'keyword': keyword,
                    'title': title,
                    'content': content,
                    'date': date,
                    'url': url
                })
                
                print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:30]}...")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœ€: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(all_data)}ê°œì˜ ì§€ì‹ì¸ ì§ˆë¬¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        return all_data
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        driver.quit()


def parse_keyword_for_display(full_keyword):
    """
    ì „ì²´ í‚¤ì›Œë“œ ë¬¸ìì—´ì—ì„œ ë©”ì¸ í‚¤ì›Œë“œì™€ + í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
    ì˜ˆ: '"ì²­ê°ì¥ì• " +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ°' -> 'ì²­ê°ì¥ì•  + ë¶ˆí¸'
    """
    if not full_keyword:
        return ""
    
    main_keyword = ""
    main_match = re.search(r'"([^"]+)"', full_keyword)
    if main_match:
        main_keyword = main_match.group(1)
    
    plus_keywords = []
    plus_pattern = r'\+\s*([^\s-]+)'
    plus_matches = re.findall(plus_pattern, full_keyword)
    plus_keywords = [kw.strip() for kw in plus_matches if kw.strip()]
    
    if main_keyword:
        if plus_keywords:
            return f"{main_keyword} + {' + '.join(plus_keywords)}"
        else:
            return main_keyword
    else:
        parts = full_keyword.split()
        if parts:
            main_keyword = parts[0].strip('"')
            if plus_keywords:
                return f"{main_keyword} + {' + '.join(plus_keywords)}"
            else:
                return main_keyword
    
    return full_keyword


def create_safe_keyword_name(parsed_keyword):
    """
    íŒŒì‹±ëœ í‚¤ì›Œë“œë¥¼ íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
    íŒŒì¼ëª… í˜•ì‹: í¬ë¡¤ë§ë°ì´í„°(ë„¤ì´ë²„ì§€ì‹ì¸, í‚¤ì›Œë“œëª…)
    """
    safe_name = parsed_keyword.replace('"', '')
    safe_name = safe_name.replace(' + ', '_+_').replace(' ', '_')
    invalid_chars = ['<', '>', ':', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        safe_name = safe_name.replace(char, '_')
    return safe_name[:50]


def save_data(all_data, keyword, source_type='ë„¤ì´ë²„ì§€ì‹ì¸'):
    """
    ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (Pickle, Excel ì €ì¥ ì œê±°)
    íŒŒì¼ëª… í˜•ì‹: í¬ë¡¤ë§ë°ì´í„°(ë„¤ì´ë²„ì§€ì‹ì¸, í‚¤ì›Œë“œëª…)
    """
    if not all_data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df_final = pd.DataFrame(all_data, columns=['keyword', 'title', 'content', 'date', 'url'])
    
    safe_keyword = create_safe_keyword_name(keyword)
    base_filename = f'í¬ë¡¤ë§ë°ì´í„°({source_type}, {safe_keyword})'
    
    csv_filename = f'{base_filename}(2).csv'
    df_final.to_csv(csv_filename, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_filename} ({len(df_final)}í–‰)")
    
    # Pickle ì €ì¥ ì œê±°
    # pkl_filename = f'{base_filename}.pkl'
    # with open(pkl_filename, 'wb') as f:
    #     pickle.dump(df_final, f)
    # print(f"âœ… Pickle ì €ì¥ ì™„ë£Œ: {pkl_filename} ({len(df_final)}í–‰)")
    
    # Excel ì €ì¥ ì œê±°
    # excel_filename = f'{base_filename}.xlsx'
    # try:
    #     df_final.to_excel(excel_filename, index=False, engine='openpyxl')
    #     print(f"âœ… Excel ì €ì¥ ì™„ë£Œ: {excel_filename} ({len(df_final)}í–‰)")
    # except Exception as e:
    #     print(f"âš ï¸  Excel ì €ì¥ ì‹¤íŒ¨: {e}")
    #     print("   openpyxl ì„¤ì¹˜ í•„ìš”: pip install openpyxl")
    
    return csv_filename


def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ë„¤ì´ë²„ ì§€ì‹ì¸ í¬ë¡¤ë§ ì‹¤í–‰
    """
    keywords = [
        '"ì²­ê°ì¥ì• " +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ì²­ê°ì¥ì• " +ê°€ì „ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ì²­ê°ì¥ì• " +ì¼ìƒ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ë†ì¸" +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ë†ì¸" +ê°€ì „ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
    ]
    
    # 2020.01.01 ~ 2022.12.31 ê¸°ê°„ ìˆ˜ì§‘
    date_ranges = calculate_date_ranges(start_date_str='20200101', end_date_str='20221231', max_urls=2500)
    
    # ë‚˜ë¨¸ì§€ ë…„ë„ ì£¼ì„ì²˜ë¦¬
    # date_ranges = calculate_date_ranges(start_date_str='20230101', end_date_str='20251115', max_urls=2500)
    
    print(f"\n{'='*60}")
    print("ë„¤ì´ë²„ ì§€ì‹ì¸ í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'='*60}")
    print(f"\nì´ {len(date_ranges)}ê°œì˜ ê¸°ê°„ìœ¼ë¡œ ë¶„í• :")
    for i, date_range in enumerate(date_ranges, 1):
        print(f"  {i}. {date_range['start']} ~ {date_range['end']} ({date_range['period']})")
    
    print(f"\nì´ {len(keywords)}ê°œì˜ í‚¤ì›Œë“œ ì¡°í•©:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw}")
    
    print(f"\ní¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    all_keywords_data = []
    
    for keyword_idx, keyword in enumerate(keywords, 1):
        print(f"\n{'='*80}")
        print(f"í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}: {keyword}")
        print(f"{'='*80}\n")
        
        keyword_all_data = []
        
        for date_range in date_ranges:
            print(f"\n{'='*60}")
            print(f"ê¸°ê°„: {date_range['start']} ~ {date_range['end']}")
            print(f"{'='*60}")
            
            all_data = crawl_naver_kin(
                keyword=keyword,
                start_date=date_range['start'],
                end_date=date_range['end'],
                max_urls=2500
            )
            if all_data:
                keyword_all_data.extend(all_data)
        
        if keyword_all_data:
            print(f"\n{'='*60}")
            print(f"í‚¤ì›Œë“œ '{keyword}' ì „ì²´ ë°ì´í„° ì €ì¥")
            print(f"{'='*60}")
            display_keyword = keyword_all_data[0]['keyword'] if keyword_all_data else keyword
            save_data(keyword_all_data, display_keyword, source_type='ë„¤ì´ë²„ì§€ì‹ì¸')
            all_keywords_data.extend(keyword_all_data)
            print(f"\nâœ… í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(keyword_all_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")
    
    print(f"\nâœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_keywords_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
