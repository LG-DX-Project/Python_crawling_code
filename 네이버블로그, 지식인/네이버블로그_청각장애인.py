# --- ë°ì´í„° ìˆ˜ì§‘(Crawling)ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from selenium import webdriver as wb  # type: ignore
from selenium.webdriver.common.by import By  # type: ignore
from selenium.webdriver.common.keys import Keys  # type: ignore
from selenium.common.exceptions import NoSuchElementException, TimeoutException  # type: ignore
from selenium.webdriver.support.ui import WebDriverWait  # type: ignore
from selenium.webdriver.support import expected_conditions as EC  # type: ignore
import re  # ì •ê·œì‹ ì‚¬ìš©

# --- ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
import pandas as pd
import time
from datetime import datetime, timedelta
from urllib.parse import quote
import pickle
import sys
import os

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

def smart_scroll(driver, scroll_step=1200, pause=0.5, container_selectors=None):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ í˜ì´ì§€ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë‹¤ì¤‘ ìŠ¤í¬ë¡¤ ì‹œë„
    """
    container_selectors = container_selectors or ['#main_pack', '#content', '#wrap', '#container', 'body', 'html']
    
    scroll_scripts = [
        ("window.scrollBy(0, arguments[0]);", scroll_step),
        ("window.scrollTo(0, document.body.scrollHeight);", None),
        ("window.scrollTo(0, document.documentElement.scrollHeight);", None)
    ]
    
    for script, value in scroll_scripts:
        try:
            if value is None:
                driver.execute_script(script)
            else:
                driver.execute_script(script, value)
            time.sleep(pause * 0.35)
        except Exception:
            continue
    
    for selector in container_selectors:
        try:
            driver.execute_script("""
                const target = document.querySelector(arguments[0]);
                const delta = arguments[1];
                if (!target) { return; }
                if (target === document.body || target === document.documentElement) {
                    window.scrollBy(0, delta);
                    window.scrollTo(0, target.scrollHeight);
                } else {
                    target.scrollTop = target.scrollHeight;
                    target.dispatchEvent(new Event('scroll', {bubbles: true}));
                }
            """, selector, scroll_step)
            time.sleep(pause * 0.2)
        except Exception:
            continue
    
    try:
        driver.execute_script("""
            window.dispatchEvent(new Event('scroll', {bubbles: true}));
            if (typeof WheelEvent !== 'undefined') {
                window.dispatchEvent(new WheelEvent('wheel', {deltaY: arguments[0], bubbles: true}));
            }
        """, scroll_step)
        time.sleep(pause * 0.15)
    except Exception:
        pass
    
    try:
        return driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);")
    except Exception:
        return None

def recover_scroll(driver, scroll_step=1200, pause=0.5):
    """
    ìŠ¤í¬ë¡¤ ë°˜ì‘ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë³´ì¡° ë™ì‘
    """
    try:
        driver.execute_script("window.scrollBy(0, -300);")
        time.sleep(pause * 0.4)
        driver.execute_script("window.scrollBy(0, arguments[0]);", max(400, scroll_step // 2))
        time.sleep(pause * 0.4)
    except Exception:
        pass

def collect_blog_urls(driver, blog_url_pattern):
    """
    í˜„ì¬ í˜ì´ì§€ì—ì„œ ë¸”ë¡œê·¸ URLì„ ìˆ˜ì§‘
    """
    temp_urls = set()
    
    try:
        title_links = driver.find_elements(By.CLASS_NAME, 'title_link')
        for link in title_links:
            try:
                href = link.get_attribute('href')
                if href and blog_url_pattern.search(href):
                    temp_urls.add(blog_url_pattern.search(href).group(0))
            except Exception:
                continue
    except Exception:
        pass
    
    try:
        blog_elements = driver.find_elements(By.CSS_SELECTOR, 'a[href^="https://blog.naver.com/"]')
        for elem in blog_elements:
            try:
                href = elem.get_attribute('href')
                if href and blog_url_pattern.search(href):
                    temp_urls.add(blog_url_pattern.search(href).group(0))
            except Exception:
                continue
    except Exception:
        pass
    
    return temp_urls

def calculate_date_ranges(start_date_str='20230101', end_date_str='20251114', max_urls=2500):
    """
    ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    - 1ë…„ ë‹¨ìœ„ë¡œ ëŠê¸°
    """
    start_date = datetime.strptime(start_date_str, '%Y%m%d')
    end_date = datetime.strptime(end_date_str, '%Y%m%d')
    date_ranges = []
    
    current_start = start_date
    while current_start < end_date:
        # 1ë…„ í›„
        current_end_1y = current_start + timedelta(days=365)
        
        # end_dateë¥¼ ë„˜ì§€ ì•Šë„ë¡
        if current_end_1y > end_date:
            current_end_1y = end_date
        
        # 1ë…„ ë‹¨ìœ„ ì‚¬ìš©
        date_ranges.append({
            'start': current_start.strftime('%Y%m%d'),
            'end': current_end_1y.strftime('%Y%m%d'),
            'period': '1year'
        })
        
        current_start = current_end_1y + timedelta(days=1)
        if current_start >= end_date:
            break
    
    return date_ranges

def crawl_naver_blog(keyword, start_date, end_date, max_urls=2500):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§
    """
    print(f"\n{'='*60}")
    print(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘")
    print(f"í‚¤ì›Œë“œ: {keyword}")
    print(f"ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"ìµœëŒ€ URL ìˆ˜: {max_urls}")
    print(f"{'='*60}\n")
    
    # URL ì¸ì½”ë”© (íŠ¹ìˆ˜ë¬¸ì í¬í•¨ í‚¤ì›Œë“œ ì œëŒ€ë¡œ ì¸ì½”ë”©)
    encoded_keyword = quote(keyword, safe='')
    
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ URL ìƒì„±
    blog_search_url = f'https://search.naver.com/search.naver?ssc=tab.blog.all&query={encoded_keyword}&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom{start_date}to{end_date}'
    
    print(f"ìƒì„±ëœ URL: {blog_search_url}\n")
    
    # í¬ë¡¬ ë¸Œë¼ìš°ì € ì‹¤í–‰ (ì˜µì…˜ ì„¤ì • - ì•ˆì •ì„± ê°•í™” ë° ë¡œê·¸ ì–µì œ)
    options = wb.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-gpu')  # GPU ê°€ì† ë¹„í™œì„±í™”
    options.add_argument('--disable-extensions')  # í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
    options.add_argument('--disable-software-rasterizer')
    options.add_argument('--disable-infobars')
    # ë¡œê·¸ ë° ì—ëŸ¬ ë©”ì‹œì§€ ì–µì œ
    options.add_argument('--log-level=3')  # INFO ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥ (FATAL, ERRORë§Œ)
    options.add_argument('--disable-logging')  # ë¡œê¹… ë¹„í™œì„±í™”
    options.add_argument('--disable-background-networking')  # ë°±ê·¸ë¼ìš´ë“œ ë„¤íŠ¸ì›Œí‚¹ ë¹„í™œì„±í™”
    options.add_argument('--disable-background-timer-throttling')  # ë°±ê·¸ë¼ìš´ë“œ íƒ€ì´ë¨¸ ì œí•œ ë¹„í™œì„±í™”
    options.add_argument('--disable-backgrounding-occluded-windows')  # ê°€ë ¤ì§„ ì°½ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ë¹„í™œì„±í™”
    options.add_argument('--disable-breakpad')  # í¬ë˜ì‹œ ë¦¬í¬íŒ… ë¹„í™œì„±í™”
    options.add_argument('--disable-component-update')  # ì»´í¬ë„ŒíŠ¸ ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”
    options.add_argument('--disable-default-apps')  # ê¸°ë³¸ ì•± ë¹„í™œì„±í™”
    options.add_argument('--disable-sync')  # ë™ê¸°í™” ë¹„í™œì„±í™”
    options.add_argument('--disable-background-mode')  # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œ ë¹„í™œì„±í™”
    options.add_argument('--disable-features=TranslateUI')  # ë²ˆì—­ UI ë¹„í™œì„±í™”
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option('prefs', {
        'profile.default_content_setting_values.notifications': 2,  # ì•Œë¦¼ ì°¨ë‹¨
        'profile.default_content_settings.popups': 0,  # íŒì—… ì°¨ë‹¨
    })
    options.add_experimental_option('useAutomationExtension', False)
    options.page_load_strategy = 'normal'  # í˜ì´ì§€ ë¡œë“œ ì „ëµ
    
    try:
        driver = wb.Chrome(options=options)
    except Exception as e:
        print(f"âš ï¸  Chrome ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹¤íŒ¨, ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„: {e}")
        driver = wb.Chrome()
    
    driver.maximize_window()
    driver.implicitly_wait(10)  # ì•”ë¬µì  ëŒ€ê¸° ì‹œê°„ ì„¤ì •
    
    # ë¸”ë¡œê·¸ URL íŒ¨í„´ ì •ê·œì‹
    blog_url_pattern = re.compile(r'https?://blog\.naver\.com/[^/]+/\d+')
    
    try:
        # í˜ì´ì§€ ì´ë™
        driver.get(blog_search_url)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        print("âœ… ë¸Œë¼ìš°ì € ì‹¤í–‰ ë° í˜ì´ì§€ ì´ë™ ì™„ë£Œ!")
        
        # "ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°" ë²„íŠ¼ í´ë¦­
        print("\nğŸ” 'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ ì°¾ëŠ” ì¤‘...")
        time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        try:
            # ì—¬ëŸ¬ ì„ íƒìë¡œ ë²„íŠ¼ ì°¾ê¸°
            detail_buttons = driver.find_elements(By.CSS_SELECTOR, 
                'a.more_link, a[class*="more_link"], a[onclick*="goOtherCR"], a[href*="ssc=tab.blog.all"]')
            
            clicked = False
            for btn in detail_buttons:
                try:
                    if btn.is_displayed() and btn.is_enabled():
                        btn_text = btn.text.strip()
                        btn_class = btn.get_attribute('class') or ''
                        if 'ìƒì„¸' in btn_text or 'ìƒì„¸ ê²€ìƒ‰' in btn_text or 'more_link' in btn_class:
                            driver.execute_script("arguments[0].click();", btn)
                            print("âœ… 'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ!")
                            time.sleep(3)  # í˜ì´ì§€ ì „í™˜ ëŒ€ê¸°
                            clicked = True
                            break
                except Exception as e:
                    continue
            
            if not clicked:
                print("âš ï¸  'ìƒì„¸ ê²€ìƒ‰ê²°ê³¼ ë³´ê¸°' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰...")
        except Exception as e:
            print(f"âš ï¸  ë²„íŠ¼ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}. ê³„ì† ì§„í–‰...")
        
        print(f"\n{'='*60}")
        print("ê³ ì • 100íšŒ ìŠ¤í¬ë¡¤ ê¸°ë°˜ URL ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'='*60}\n")
        
        fixed_scroll_count = 100
        scroll_pause = 1.0
        
        time.sleep(2)  # ìŠ¤í¬ë¡¤ ì‹œì‘ ì „ ì¶”ê°€ ëŒ€ê¸°
        try:
            body = driver.find_element(By.TAG_NAME, 'body')
        except Exception:
            body = None
        
        # ìŠ¤í¬ë¡¤ ë†’ì´ ì¶”ì  ë³€ìˆ˜
        last_height = driver.execute_script("return document.body.scrollHeight")
        no_change_count = 0  # ìŠ¤í¬ë¡¤ ë†’ì´ê°€ ë³€í•˜ì§€ ì•Šì€ íšŸìˆ˜
        
        print("  ğŸ“œ ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì¤‘...")
        for scroll_round in range(fixed_scroll_count):
            actual_scrolls = scroll_round + 1
            try:
                if body is None:
                    body = driver.find_element(By.TAG_NAME, 'body')
                body.send_keys(Keys.END)
            except Exception:
                try:
                    body = driver.find_element(By.TAG_NAME, 'body')
                    body.send_keys(Keys.END)
                except Exception:
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)
            
            # ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                no_change_count += 1
                if no_change_count >= 5:  # 5ë²ˆ ì—°ì† ë³€í™” ì—†ìœ¼ë©´ ì¤‘ë‹¨
                    print(f"  âš ï¸  ìŠ¤í¬ë¡¤ ë†’ì´ ë³€í™” ì—†ìŒ ({actual_scrolls}/{fixed_scroll_count}ë²ˆì§¸ ìŠ¤í¬ë¡¤). ì¡°ê¸° ì¢…ë£Œ.")
                    break
            else:
                no_change_count = 0
                last_height = new_height
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ë²ˆë§ˆë‹¤)
            if (scroll_round + 1) % 10 == 0:
                print(f"    ì§„í–‰: {scroll_round + 1}/{fixed_scroll_count}ë²ˆ ìŠ¤í¬ë¡¤ ì™„ë£Œ")
                try:
                    more_buttons = driver.find_elements(By.CSS_SELECTOR, 'a.more, button.more, .more, [class*="more"], [class*="btn_more"]')
                    for btn in more_buttons:
                        try:
                            if btn.is_displayed() and btn.is_enabled():
                                driver.execute_script("arguments[0].click();", btn)
                                time.sleep(1)
                                print("  ğŸ”„ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­")
                                break
                        except Exception:
                            continue
                except Exception:
                    pass
        
        print("  âœ… ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì™„ë£Œ!")
        
        # ìŠ¤í¬ë¡¤ ì™„ë£Œ í›„ ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
        print("  â³ ìµœì¢… ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(2)
        
        # ìŠ¤í¬ë¡¤ ì™„ë£Œ í›„ í•œ ë²ˆì— URL ìˆ˜ì§‘
        print("  ğŸ” URL ìˆ˜ì§‘ ì¤‘...")
        all_seen_urls = collect_blog_urls(driver, blog_url_pattern)
        href_list = list(all_seen_urls)
        
        print(f"\nâœ… ì´ {len(href_list)}ê°œì˜ ë¸”ë¡œê·¸ URL ìˆ˜ì§‘ ì™„ë£Œ! (ì´ {actual_scrolls}ë²ˆ ìŠ¤í¬ë¡¤)")
        
        # URL ìˆ˜ê°€ 1000ê°œ ì´í•˜ë©´ 1ë…„ ë‹¨ìœ„ë¡œ ë³€ê²½í•˜ë¼ëŠ” ë©”ì‹œì§€
        if len(href_list) < 1000:
            print(f"\nâš ï¸  URL ìˆ˜ê°€ 1000ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤ ({len(href_list)}ê°œ).")
            print("   1ë…„ ë‹¨ìœ„ë¡œ ê¸°ê°„ì„ í™•ì¥í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ìµœëŒ€ URL ìˆ˜ ì œí•œ
        if len(href_list) > max_urls:
            href_list = href_list[:max_urls]
            print(f"âš ï¸  ìµœëŒ€ URL ìˆ˜({max_urls})ë¡œ ì œí•œ: {len(href_list)}ê°œ")
        
        # ê° ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        print(f"\n{'='*60}")
        print("ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
        print(f"{'='*60}\n")
        
        all_data = []
        
        for i, url in enumerate(href_list, 1):
            try:
                print(f"[{i}/{len(href_list)}] ì²˜ë¦¬ ì¤‘: {url[:60]}...")
                driver.get(url)
                time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ë³¸ë¬¸ì´ 'mainFrame'ì´ë¼ëŠ” iframe ì•ˆì— ìˆìŒ
                try:
                    driver.switch_to.frame('mainFrame')
                except:
                    pass  # iframeì´ ì—†ì„ ìˆ˜ë„ ìˆìŒ
                
                # ì œëª© ì¶”ì¶œ (ì‹ í˜•/êµ¬í˜• ì—ë””í„° ëª¨ë‘ ì‹œë„)
                title = "N/A"
                title_selectors = [
                    (By.CSS_SELECTOR, '.se-title-text'),
                    (By.CSS_SELECTOR, '.pcol1 > span'),
                    (By.CSS_SELECTOR, '.se-title'),
                    (By.TAG_NAME, 'h1')
                ]
                
                for selector_type, selector in title_selectors:
                    try:
                        title_elem = driver.find_element(selector_type, selector)
                        title = title_elem.text.strip()
                        if title and title != "N/A":
                            break
                    except:
                        continue
                
                # ë³¸ë¬¸ ì¶”ì¶œ (ì‹ í˜•/êµ¬í˜• ì—ë””í„° ëª¨ë‘ ì‹œë„)
                content = "N/A"
                content_selectors = [
                    (By.CSS_SELECTOR, '.se-main-container'),
                    (By.CSS_SELECTOR, '#postViewArea'),
                    (By.CSS_SELECTOR, '.se-component-content'),
                    (By.CSS_SELECTOR, '.post-view')
                ]
                
                for selector_type, selector in content_selectors:
                    try:
                        content_elem = driver.find_element(selector_type, selector)
                        content = content_elem.text.strip()
                        # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
                        content = content.replace('\n', ' ').replace('\r', ' ')
                        # ì—°ì†ëœ ê³µë°± ì œê±°
                        content = ' '.join(content.split())
                        if content and len(content) > 10:
                            break
                    except:
                        continue
                
                # ë‚ ì§œ ì¶”ì¶œ (ì‹ í˜•/êµ¬í˜• ì—ë””í„° ëª¨ë‘ ì‹œë„)
                date = "N/A"
                date_selectors = [
                    (By.CSS_SELECTOR, '.se_publishDate'),
                    (By.CSS_SELECTOR, '.date'),
                    (By.CSS_SELECTOR, '.publish_date'),
                    (By.CSS_SELECTOR, '.post-date')
                ]
                
                for selector_type, selector in date_selectors:
                    try:
                        date_elem = driver.find_element(selector_type, selector)
                        date = date_elem.text.strip()
                        if date and date != "N/A":
                            # ë‚ ì§œ í˜•ì‹ ì •ë¦¬ (ì˜ˆ: 2025.01.15. ì˜¤í›„ 3:00 -> 2025.01.15.)
                            if '.' in date:
                                date_parts = date.split('.')
                                if len(date_parts) >= 3:
                                    date = f"{date_parts[0]}.{date_parts[1]}.{date_parts[2]}."
                            break
                    except:
                        continue
                
                # iframeì—ì„œ ë¹ ì ¸ë‚˜ì™€ ê¸°ë³¸ ì½˜í…ì¸ ë¡œ ì „í™˜
                driver.switch_to.default_content()
                
                # contentê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìœ¼ë©´ ì œì™¸
                if content == "N/A" or not content or len(content.strip()) < 20:
                    print(f"  â­ï¸  content ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒìœ¼ë¡œ ê±´ë„ˆëœ€: {title[:30]}...")
                    continue
                
                # titleì´ ì—†ìœ¼ë©´ ì œì™¸
                if title == "N/A" or not title or len(title.strip()) == 0:
                    print(f"  â­ï¸  title ì—†ìŒìœ¼ë¡œ ê±´ë„ˆëœ€")
                    continue
                
                # ë°ì´í„° ì €ì¥ (keyword, title, content, date, url ìˆœì„œ)
                all_data.append({
                    'keyword': keyword,
                    'title': title,
                    'content': content,
                    'date': date,
                    'url': url
                })
                
                print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:30]}...")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœ€: {e}")
                # iframeì—ì„œ ë¹ ì ¸ë‚˜ì˜¤ê¸°
                try:
                    driver.switch_to.default_content()
                except:
                    pass
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°ì´í„° ì €ì¥í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€
                continue
        
        print(f"\nâœ… ì´ {len(all_data)}ê°œì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        return all_data
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        driver.quit()

def clean_keyword(keyword):
    """
    keywordì—ì„œ ì œì™¸ ì—°ì‚°ì(-)ì™€ ì œì™¸ í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ê³ ,
    í¬í•¨ í‚¤ì›Œë“œì™€ + ì—°ì‚°ìë§Œ ìœ ì§€
    ì˜ˆ: "ì²­ê°ì¥ì• " +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ° -> "ì²­ê°ì¥ì• " +ë¶ˆí¸
    """
    if pd.isna(keyword) or not keyword:
        return keyword
    
    # ë¬¸ìì—´ë¡œ ë³€í™˜
    keyword_str = str(keyword)
    
    # ì œì™¸ ì—°ì‚°ì(-)ì™€ ê·¸ ë’¤ì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    # - ì—°ì‚°ìê°€ ë‚˜íƒ€ë‚˜ëŠ” ì²« ë²ˆì§¸ ìœ„ì¹˜ë¥¼ ì°¾ì•„ì„œ ê·¸ ì•ë¶€ë¶„ë§Œ ìœ ì§€
    if ' -' in keyword_str:
        keyword_str = keyword_str.split(' -')[0]
    elif keyword_str.startswith('-'):
        # -ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ëŠ” ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì´ìƒí•œ ê²½ìš°)
        return keyword_str
    
    # ì•ë’¤ ê³µë°± ì œê±°
    keyword_str = keyword_str.strip()
    
    return keyword_str

def create_safe_keyword_name(keyword):
    """
    í‚¤ì›Œë“œë¥¼ íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
    íŒŒì¼ëª… í˜•ì‹: í¬ë¡¤ë§ë°ì´í„°(ë„¤ì´ë²„ë¸”ë¡œê·¸, í‚¤ì›Œë“œëª…)
    """
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì²˜ë¦¬
    safe_name = keyword.replace('"', '').replace('+', 'í¬í•¨').replace('-', 'ì œì™¸')
    safe_name = safe_name.replace(' ', '_').replace('|', '_')
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±° (ë‹¨, íŒŒì¼ëª… ìì²´ì˜ ê´„í˜¸ëŠ” ìœ ì§€)
    invalid_chars = ['<', '>', ':', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        safe_name = safe_name.replace(char, '_')
    # ë„ˆë¬´ ê¸´ íŒŒì¼ëª… ë°©ì§€
    return safe_name[:50]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ

def save_data(all_data, keyword, source_type='ë„¤ì´ë²„ë¸”ë¡œê·¸'):
    """
    ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    íŒŒì¼ëª…: ë„¤ì´ë²„ë¸”ë¡œê·¸_ì²­ê°ì¥ì• (2).csv
    """
    if not all_data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # DataFrame ìƒì„± (keyword, title, content, date, url ìˆœì„œ)
    df_final = pd.DataFrame(all_data, columns=['keyword', 'title', 'content', 'date', 'url'])
    
    # keyword ì»¬ëŸ¼ ì •ë¦¬ (ì œì™¸ í‚¤ì›Œë“œ ì œê±°)
    df_final['keyword'] = df_final['keyword'].apply(clean_keyword)
    
    # íŒŒì¼ëª…: ë„¤ì´ë²„ë¸”ë¡œê·¸_ì²­ê°ì¥ì• (2).csv
    csv_filename = 'ë„¤ì´ë²„ë¸”ë¡œê·¸_ì²­ê°ì¥ì• (2).csv'
    
    # CSV ì €ì¥ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ append, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
    if os.path.exists(csv_filename):
        # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
        df_existing = pd.read_csv(csv_filename, encoding='utf-8-sig')
        # ê¸°ì¡´ íŒŒì¼ì˜ keywordë„ ì •ë¦¬
        if 'keyword' in df_existing.columns:
            df_existing['keyword'] = df_existing['keyword'].apply(clean_keyword)
        # ìƒˆ ë°ì´í„°ì™€ í†µí•©
        df_final = pd.concat([df_existing, df_final], ignore_index=True)
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        df_final = df_final.drop_duplicates(subset=['url'], keep='first')
        print(f"  ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.")
    
    df_final.to_csv(csv_filename, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_filename} ({len(df_final)}í–‰)")
    
    return csv_filename

def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹¤í–‰
    """
    # í‚¤ì›Œë“œ ëª©ë¡ ì„¤ì •
    keywords = [
        '"ì²­ê°ì¥ì• " +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ì²­ê°ì¥ì• " +ê°€ì „ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ì²­ê°ì¥ì• " +ì¼ìƒ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ë†ì¸" +ë¶ˆí¸ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
        '"ë†ì¸" +ê°€ì „ -ì•Œë¦¬ -ê´‘êµ° -ê´‘ê³  -ì¿ í° -ë³´ì²­ê¸° -ì¸ê³µì™€ìš° -ì£¼ë¯¼ì„¼í„° -ì‚°ì¬ -ì‹ ì²­',
    ]
    
    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (2020.01.01 ~ 2022.12.31, 1ë…„ ë‹¨ìœ„)
    date_ranges = calculate_date_ranges(start_date_str='20200101', end_date_str='20221231', max_urls=2500)
    
    # 2023.01.01 ì´í›„ ë°ì´í„°ëŠ” ì´ë¯¸ ìˆ˜ì§‘í–ˆìœ¼ë¯€ë¡œ ì£¼ì„ì²˜ë¦¬
    # date_ranges_2023 = calculate_date_ranges(start_date_str='20230101', end_date_str='20251114', max_urls=2500)
    
    print(f"\n{'='*60}")
    print("ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'='*60}")
    print(f"\nì´ {len(date_ranges)}ê°œì˜ ê¸°ê°„ìœ¼ë¡œ ë¶„í• :")
    for i, date_range in enumerate(date_ranges, 1):
        print(f"  {i}. {date_range['start']} ~ {date_range['end']} ({date_range['period']})")
    
    print(f"\nì´ {len(keywords)}ê°œì˜ í‚¤ì›Œë“œ ì¡°í•©:")
    for i, kw in enumerate(keywords, 1):
        print(f"  {i}. {kw}")
    
    print(f"\ní¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # ê° í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
    all_keywords_data = []
    
    for keyword_idx, keyword in enumerate(keywords, 1):
        print(f"\n{'='*80}")
        print(f"í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}: {keyword}")
        print(f"{'='*80}\n")
        
        keyword_all_data = []
        
        for date_range in date_ranges:
            print(f"\n{'='*60}")
            print(f"ê¸°ê°„: {date_range['start']} ~ {date_range['end']}")
            print(f"{'='*60}")
            
            all_data = crawl_naver_blog(
                keyword=keyword,
                start_date=date_range['start'],
                end_date=date_range['end'],
                max_urls=2500
            )
            if all_data:
                keyword_all_data.extend(all_data)
        
        # ê° í‚¤ì›Œë“œë³„ ì „ì²´ ë°ì´í„° í†µí•© ì €ì¥ (CSV, Excel, Pickle ë™ì¼ ë°ì´í„°)
        if keyword_all_data:
            print(f"\n{'='*60}")
            print(f"í‚¤ì›Œë“œ '{keyword}' ì „ì²´ ë°ì´í„° ì €ì¥")
            print(f"{'='*60}")
            save_data(keyword_all_data, keyword, source_type='ë„¤ì´ë²„ë¸”ë¡œê·¸')
            all_keywords_data.extend(keyword_all_data)
            print(f"\nâœ… í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(keyword_all_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")
    
    print(f"\nâœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_keywords_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
